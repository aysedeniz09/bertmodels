{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd542fea-4874-426b-adce-24b9e72c2c49",
   "metadata": {},
   "source": [
    "# BERTOPIC & ANTMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc21094-35aa-4e15-ab46-0b8f2b388cbc",
   "metadata": {},
   "source": [
    "### The following code is an example of combining (Bertopic)[https://github.com/MaartenGr/BERTopic] with ANTMN Methodology (Walter & Ophir 2019)[https://github.com/DrorWalt/ANTMN]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f3b8a-bbca-4553-8171-3e07406be70a",
   "metadata": {},
   "source": [
    "### First import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d552c-05d0-4750-9087-b7f56ccc6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import scipy\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "import sys   \n",
    "import unicodedata\n",
    "import nltk \n",
    "import numpy as np   \n",
    "import hdbscan\n",
    "import time     \n",
    "from scipy.sparse import csr_matrix, csc_matrix \n",
    "from umap import UMAP\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e0990-a778-4991-a55c-b2fa31001986",
   "metadata": {},
   "source": [
    "### Load the data from github link, always check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431a98b-f60d-4a09-86ac-5c812185d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/aysedeniz09/bertmodels/main/data/Data_Class_ADL.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42afb6-6343-4373-a03e-2038b884c218",
   "metadata": {},
   "source": [
    "### Load the embedding model you will use, for more options check (hugging face transformers)[https://huggingface.co/models]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c77efd-d076-474d-9b3f-22bc3361acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606ac5b-cf24-4be4-b886-2eca5a8086e2",
   "metadata": {},
   "source": [
    "### Create a function to clean the text, in this step you can add stopwords as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a04e7d-82a4-4f8f-b85e-ca39910a3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x):\n",
    "\n",
    "    ### Light\n",
    "    x = x.lower() # lowercase everything\n",
    "    x = x.encode('ascii', 'ignore').decode()  # remove unicode characters\n",
    "    x = re.sub(r'https*\\S+', ' ', x) # remove links\n",
    "    x = re.sub(r'http*\\S+', ' ', x)\n",
    "    # cleaning up text\n",
    "    x = re.sub(r'\\'\\w+', '', x) \n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n",
    "    \n",
    "    ### Heavy\n",
    "    x = re.sub(r'@\\S', '', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    # remove single letters and numbers surrounded by space\n",
    "    x = re.sub(r'\\s[a-z]\\s|\\s[0-9]\\s', ' ', x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdeeb1-3507-4fc9-a683-ea0072a3ac5a",
   "metadata": {},
   "source": [
    "### Now first drop empty rows, then change the data to strings preparing for BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360cc6d-ca69-4a07-bc59-5dc789f54434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df #create a backup folder\n",
    "train.dropna(subset=['originaltext'])\n",
    "nan_value = float(\"NaN\")\n",
    "train.replace(\"\", nan_value, inplace=True)\n",
    "train.dropna(subset = [\"originaltext\"], inplace=True)\n",
    "train.replace(\" \", nan_value, inplace=True)\n",
    "train.dropna(subset = [\"originaltext\"], inplace=True)\n",
    "train.info() # check the dataframe\n",
    "train.head() # again check the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66e50b-0370-4adb-ab21-57c1584d5227",
   "metadata": {},
   "source": [
    "### Apply clean text function, and change the original text variable to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1a12e-162a-4d38-94bd-9e67d1c3858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text'] = train.originaltext.apply(text_clean)\n",
    "traintext = train.cleaned_text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93e529-1f94-47d5-89cc-a33eaad7d48e",
   "metadata": {},
   "source": [
    "### Start the BERTopic, to run ANTMN afterwards **calculate_probabilities = True** must be TRUE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc844d-a2f2-4041-bd54-da5df47c3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # to checktime\n",
    "umap_model = UMAP(n_neighbors=2, n_components=2, \n",
    "                  min_dist=0.0, metric='cosine', random_state=5) #umap fixes the BERTopic so it can be replicated\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=roberta, nr_topics=\"20\", calculate_probabilities = True).fit(traintext)\n",
    "#for this example I have set the nr_topics as 20, however usually it is recommended to leave it as auto and let BERTopic find the optimal topics\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a698c76-0896-4ed8-9d8a-7b81b6ce61a8",
   "metadata": {},
   "source": [
    "### Map the probabilities using HDBSCAN clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a1586-67f0-4302-a65f-0e83c5a152ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # to check time\n",
    "probs = hdbscan.all_points_membership_vectors(topic_model.hdbscan_model) # clustering algorithm\n",
    "probs = topic_model._map_probabilities(probs, original_topics=True) # this is the document that will be used for ANTMN\n",
    "topics, probs = topic_model.fit_transform(traintext)\n",
    "df_prob = pd.DataFrame(probs) # THIS IS THE DOCUMENT THAT WILL BE USED IN ANTMN\n",
    "#topic_model.save(\"Bert_Model_Outputs/antmn_sample_v1\") #to save the model for future\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b353c3e-7ee5-46d7-827a-f7a799663938",
   "metadata": {},
   "source": [
    "### At this step you can save the df_prob as a csv and switch to the R code and run ANTMN on R, or continue in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d0d40-4b13-4bc2-9f46-b14a0077b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob.to_csv(\"BERTopic_ANTMN_Probabilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f213d6-582f-460b-ac70-1c2eb5f1d8a5",
   "metadata": {},
   "source": [
    "### Save document of topic names and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c59d63-f9b9-483f-9b1f-4aab95ab95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info() \n",
    "freq.to_csv(\"BERTopic_ANTMN_TopicNamesandFreq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d29dc8-1321-43ac-9c87-3d63638870fc",
   "metadata": {},
   "source": [
    "# BERTopic & ANTMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31772893-ce5a-43e0-80f1-93364cc9c70c",
   "metadata": {},
   "source": [
    "## The method is from the supplemental code, citation: Walter, D., & Ophir, Y. (2019). News Frame Analysis: An Inductive Mixed-Method Computational Approach. Communication Methods and Measures. https://doi.org/10.1080/19312458.2019.1639145."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afffe9-4f1f-4033-ac5e-0edd6cc069b5",
   "metadata": {},
   "source": [
    "**Note: Due to the nature of BERTopic different than LDA, not all documents are connected with each other. Therefore had to remove SpinGlass algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e4795-734d-491f-9ec2-53758246aaf6",
   "metadata": {},
   "source": [
    "## (R Code)[https://github.com/aysedeniz09/bertmodels/blob/f27341708deff031832b70d1dd3d3cba4a13ad2b/bert_antmn_R.md] to run ANTMN on the BERTopic objects, to continue in python follow the below steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
