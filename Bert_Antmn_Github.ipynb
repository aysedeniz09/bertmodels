{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f5916e-8b41-49b1-972a-fbbc84b7e6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERTOPIC & ANTMN \n",
    "The following code is an example of combining [Bertopic](https://github.com/MaartenGr/BERTopic) with ANTMN Methodology [(Walter & Ophir 2019)](https://github.com/DrorWalt/ANTMN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49105e89-7ff8-4971-aae0-7d9de2dde5f7",
   "metadata": {},
   "source": [
    "1. First import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad245785-4458-4a23-b3b6-177831e2dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import scipy\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "import sys   \n",
    "import unicodedata\n",
    "import nltk \n",
    "import numpy as np   \n",
    "import hdbscan\n",
    "import time     \n",
    "from scipy.sparse import csr_matrix, csc_matrix \n",
    "from umap import UMAP\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1920b-dee5-43ca-bfd0-f6c8b9bb83a7",
   "metadata": {},
   "source": [
    "2. Load the data from github link, always check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9959d14b-32d4-499a-b784-efb84ea51ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   index        date  source.domain  \\\n",
      "0           1   98474  2021-11-21    foxnews.com   \n",
      "1           2   23319  2021-07-29    foxnews.com   \n",
      "2           3  144569  2021-04-16  dailywire.com   \n",
      "3           4   38059  2021-08-26      abc13.com   \n",
      "4           5   97919  2021-11-28     silive.com   \n",
      "\n",
      "                                        originaltext  \n",
      "0  chicago mayor needs to dump police boss if <U+...  \n",
      "1  randi weingarten ripped after telling msnbc 'w...  \n",
      "2  pfizer ceo: third covid vaccine dose <U+0091>l...  \n",
      "3  texas a&m researchers develop treatment to hel...  \n",
      "4  nyc civil service exam: these applications are...  \n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/aysedeniz09/bertmodels/main/data/Data_Class_ADL.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282753af-8472-44f9-aca2-02e0540053d5",
   "metadata": {},
   "source": [
    "3. Load the embedding model you will use, for more options check (hugging face transformers)[https://huggingface.co/models]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa46c181-2070-48aa-96bb-c6d610cfc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4290c-1711-4d2c-a1b2-e1c7ba98f247",
   "metadata": {},
   "source": [
    "4. Create a function to clean the text, in this step you can add stopwords as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74eb0f59-c393-49f1-937e-d6c1f71154a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x):\n",
    "\n",
    "    ### Light\n",
    "    x = x.lower() # lowercase everything\n",
    "    x = x.encode('ascii', 'ignore').decode()  # remove unicode characters\n",
    "    x = re.sub(r'https*\\S+', ' ', x) # remove links\n",
    "    x = re.sub(r'http*\\S+', ' ', x)\n",
    "    # cleaning up text\n",
    "    x = re.sub(r'\\'\\w+', '', x) \n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n",
    "    \n",
    "    ### Heavy\n",
    "    x = re.sub(r'@\\S', '', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    # remove single letters and numbers surrounded by space\n",
    "    x = re.sub(r'\\s[a-z]\\s|\\s[0-9]\\s', ' ', x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276a787-d6db-4dbb-8dd0-3132c3fc6d91",
   "metadata": {},
   "source": [
    "5. Now first drop empty rows, then change the data to strings preparing for BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8987a8bd-86c9-4404-97b9-af710946d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     1500 non-null   int64 \n",
      " 1   index          1500 non-null   int64 \n",
      " 2   date           1500 non-null   object\n",
      " 3   source.domain  1500 non-null   object\n",
      " 4   originaltext   1500 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 58.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>source.domain</th>\n",
       "      <th>originaltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>98474</td>\n",
       "      <td>2021-11-21</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>chicago mayor needs to dump police boss if &lt;U+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>23319</td>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>randi weingarten ripped after telling msnbc 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>144569</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>dailywire.com</td>\n",
       "      <td>pfizer ceo: third covid vaccine dose &lt;U+0091&gt;l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38059</td>\n",
       "      <td>2021-08-26</td>\n",
       "      <td>abc13.com</td>\n",
       "      <td>texas a&amp;m researchers develop treatment to hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>97919</td>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>silive.com</td>\n",
       "      <td>nyc civil service exam: these applications are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   index        date  source.domain  \\\n",
       "0           1   98474  2021-11-21    foxnews.com   \n",
       "1           2   23319  2021-07-29    foxnews.com   \n",
       "2           3  144569  2021-04-16  dailywire.com   \n",
       "3           4   38059  2021-08-26      abc13.com   \n",
       "4           5   97919  2021-11-28     silive.com   \n",
       "\n",
       "                                        originaltext  \n",
       "0  chicago mayor needs to dump police boss if <U+...  \n",
       "1  randi weingarten ripped after telling msnbc 'w...  \n",
       "2  pfizer ceo: third covid vaccine dose <U+0091>l...  \n",
       "3  texas a&m researchers develop treatment to hel...  \n",
       "4  nyc civil service exam: these applications are...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df #create a backup folder\n",
    "train.dropna(subset=['originaltext'])\n",
    "nan_value = float(\"NaN\")\n",
    "train.replace(\"\", nan_value, inplace=True)\n",
    "train.dropna(subset = [\"originaltext\"], inplace=True)\n",
    "train.replace(\" \", nan_value, inplace=True)\n",
    "train.dropna(subset = [\"originaltext\"], inplace=True)\n",
    "train.info() # check the dataframe\n",
    "train.head() # again check the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd65bfa-27af-44d1-8da9-b71d382f1677",
   "metadata": {},
   "source": [
    "6. Apply clean text function, and change the original text variable to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87c8ecf-0bca-4853-8866-04721c8d3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text'] = train.originaltext.apply(text_clean)\n",
    "traintext = train.cleaned_text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef7a94-8f32-4375-ae90-fd856a98f098",
   "metadata": {},
   "source": [
    "6. Start the BERTopic, to run ANTMN afterwards **calculate_probabilities = True** must be TRUE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9bc8f4-087a-45c7-a502-7b35c714fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 230.85283041000366 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # to checktime\n",
    "umap_model = UMAP(n_neighbors=2, n_components=2, \n",
    "                  min_dist=0.0, metric='cosine', random_state=5) #umap fixes the BERTopic so it can be replicated\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=roberta, nr_topics=\"20\", calculate_probabilities = True).fit(traintext)\n",
    "#for this example I have set the nr_topics as 20, however usually it is recommended to leave it as auto and let BERTopic find the optimal topics\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c499d-9add-48ca-8498-effea32b6b44",
   "metadata": {},
   "source": [
    "7. Map the probabilities using HDBSCAN clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd6a82c-24c2-418a-a38c-5e1f800ab430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 240.29081916809082 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # to check time\n",
    "probs = hdbscan.all_points_membership_vectors(topic_model.hdbscan_model) # clustering algorithm\n",
    "probs = topic_model._map_probabilities(probs, original_topics=True) # this is the document that will be used for ANTMN\n",
    "topics, probs = topic_model.fit_transform(traintext)\n",
    "df_prob = pd.DataFrame(probs) # THIS IS THE DOCUMENT THAT WILL BE USED IN ANTMN\n",
    "#topic_model.save(\"Bert_Model_Outputs/antmn_sample_v1\") #to save the model for future\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d90e3-0181-4223-93d0-8affe829e04e",
   "metadata": {},
   "source": [
    "8. At this step you can save the *df_prob* as a csv and switch to the R code and run ANTMN on R, or continue in this script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0e210f-d577-4891-bc4f-f258ab22a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob.to_csv(\"BERTopic_ANTMN_Probabilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4910f-be2a-4f3c-aefc-13fd9deec5cf",
   "metadata": {},
   "source": [
    "9. Save document of topic names and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c7b8e5-1c1e-4554-aa79-499c33c2eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info() \n",
    "freq.to_csv(\"BERTopic_ANTMN_TopicNamesandFreq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d1702-4a72-4b01-9a77-1afec12128cc",
   "metadata": {},
   "source": [
    "## BERTopic & ANTMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd8c56-1930-451a-b774-5e81f12a9959",
   "metadata": {},
   "source": [
    "The method is from the supplemental code, citation: Walter, D., & Ophir, Y. (2019). News Frame Analysis: An Inductive Mixed-Method Computational Approach. Communication Methods and Measures. https://doi.org/10.1080/19312458.2019.1639145."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160c40b-3536-4c66-adca-c8f1e558929e",
   "metadata": {},
   "source": [
    "### Note: Due to the nature of BERTopic different than LDA, not all documents are connected with each other. Therefore had to remove SpinGlass algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89339de4-7ce9-4d4f-87c1-4201e4e861a5",
   "metadata": {},
   "source": [
    "1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1d61eb8-a27f-41e3-92e6-091e24021a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting igraph\n",
      "  Downloading igraph-0.10.1-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "Collecting texttable>=1.6.2\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, igraph\n",
      "Successfully installed igraph-0.10.1 texttable-1.6.4\n",
      "Collecting latent-semantic-analysis\n",
      "  Downloading latent_semantic_analysis-0.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from latent-semantic-analysis) (5.4.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from latent-semantic-analysis) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from latent-semantic-analysis) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from latent-semantic-analysis) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.4 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from latent-semantic-analysis) (1.7.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->latent-semantic-analysis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->latent-semantic-analysis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->latent-semantic-analysis) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.2->latent-semantic-analysis) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\adl6244\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.2->latent-semantic-analysis) (2.2.0)\n",
      "Installing collected packages: latent-semantic-analysis\n",
      "Successfully installed latent-semantic-analysis-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install igraph\n",
    "!pip install latent-semantic-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68460f94-6e02-4d98-bf3f-d8ee35789d32",
   "metadata": {},
   "source": [
    "2. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb29d9c-589c-4ec1-880f-3af2030f9668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f81b8bc-18a9-487b-b789-ee32ca82444f",
   "metadata": {},
   "source": [
    "3. Write the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acb576-907b-4a26-863d-d8555afdd089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
